{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9c70133c",
      "metadata": {
        "papermill": {
          "duration": 0.006596,
          "end_time": "2024-12-19T13:42:56.775900",
          "exception": false,
          "start_time": "2024-12-19T13:42:56.769304",
          "status": "completed"
        },
        "tags": [],
        "id": "9c70133c"
      },
      "source": [
        "# Evaluation Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c49078",
      "metadata": {
        "papermill": {
          "duration": 0.006815,
          "end_time": "2024-12-19T13:42:56.789923",
          "exception": false,
          "start_time": "2024-12-19T13:42:56.783108",
          "status": "completed"
        },
        "tags": [],
        "id": "11c49078"
      },
      "source": [
        "---\n",
        "Submissions are evaluated using the **Quadratic Weighted Kappa (QWK)**, a metric designed to measure agreement between predicted and actual outcomes. This metric is particularly suitable for ordinal regression tasks, where the relative ordering of classes carries importance. The QWK score typically ranges from 0 (indicating random agreement) to 1 (indicating perfect agreement). However, if the agreement is worse than expected by chance, the metric may produce a negative score.\n",
        "\n",
        "***Computation of Quadratic Weighted Kappa***\n",
        "\n",
        "To calculate QWK, three matrices are constructed: $O$, $W$, and $E$, where $N$ represents the number of distinct labels.\n",
        "\n",
        "1. **Matrix $O$ (Observed Outcomes):**\n",
        "   - $O$ is an $N \\times N$ histogram matrix.\n",
        "   - Each element $O_{i,j}$ represents the count of instances where the actual value is $i$ and the predicted value is $j$.\n",
        "\n",
        "2. **Matrix $W$ (Weight Matrix):**\n",
        "   - $W$ is an $N \\times N$ matrix that assigns a penalty based on the squared difference between actual and predicted values.\n",
        "   - The weights are calculated as:\n",
        "\n",
        "     $$W_{i,j} = \\frac{(i - j)^2}{(N - 1)^2},$$\n",
        "     \n",
        "     where larger differences between $i$ and $j$ result in higher penalties.\n",
        "\n",
        "3. **Matrix $E$ (Expected Outcomes):**\n",
        "   - $E$ is an $N \\times N$ matrix representing the expected outcomes under the assumption of no correlation between actual and predicted labels.\n",
        "   - It is computed as the outer product of the histogram vectors of actual and predicted labels, normalized so that $E$ and $O$ have the same total sum:\n",
        "\n",
        "     $$E_{i,j} = \\frac{\\text{Actual Histogram}_i \\cdot \\text{Predicted Histogram}_j}{\\sum O_{i,j}}.$$\n",
        "\n",
        "***Formula for Quadratic Weighted Kappa***\n",
        "\n",
        "Using these matrices, QWK is computed as:\n",
        "\n",
        "$$\\kappa = 1 - \\frac{\\sum_{i,j} W_{i,j} \\cdot O_{i,j}}{\\sum_{i,j} W_{i,j} \\cdot E_{i,j}}.$$\n",
        "\n",
        "This formula represents one minus the ratio of the observed weighted error to the expected weighted error. A higher $\\kappa$ value indicates better alignment between the predicted and actual labels, relative to random predictions.\n",
        "\n",
        "***Interpretation***\n",
        "\n",
        "- **$\\kappa = 1$: Perfect agreement.**\n",
        "- **$\\kappa = 0$: Agreement is purely random.**\n",
        "- **$\\kappa < 0$: Agreement is worse than random.**\n",
        "\n",
        "By penalizing misclassifications based on the magnitude of their deviation, the QWK metric provides a robust evaluation of predictive performance in ordinal classification problems. It is especially effective when dealing with imbalanced datasets or scenarios where the relative distance between classes is significant.\n",
        "\n",
        "For additional details on the Quadratic Weighted Kappa, refer to [this tutorial](https://datatab.net/tutorial/weighted-cohens-kappa).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd33ba9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:42:56.804492Z",
          "iopub.status.busy": "2024-12-19T13:42:56.804196Z",
          "iopub.status.idle": "2024-12-19T13:43:17.121748Z",
          "shell.execute_reply": "2024-12-19T13:43:17.120967Z"
        },
        "papermill": {
          "duration": 20.327034,
          "end_time": "2024-12-19T13:43:17.123762",
          "exception": false,
          "start_time": "2024-12-19T13:42:56.796728",
          "status": "completed"
        },
        "tags": [],
        "id": "dbd33ba9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import copy\n",
        "import pickle\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.optimize import minimize\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.optimizers import Adam\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from colorama import Fore, Style\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b778df",
      "metadata": {
        "papermill": {
          "duration": 0.020665,
          "end_time": "2024-12-19T13:45:17.096213",
          "exception": false,
          "start_time": "2024-12-19T13:45:17.075548",
          "status": "completed"
        },
        "tags": [],
        "id": "86b778df"
      },
      "source": [
        "---\n",
        "\n",
        "### Issues with the Quadratic Weighted Kappa Metric\n",
        "\n",
        "Despite its widespread use, the Quadratic Weighted Kappa metric has notable limitations that can affect its utility in evaluating model performance:\n",
        "\n",
        "1. **Non-Intuitive Behavior:**\n",
        "   - In some cases, predicting incorrect values may result in a better QWK score than predicting the actual values. This counterintuitive behavior complicates the interpretation of the metric and raises questions about whether improvements in QWK scores reflect genuine model enhancements.\n",
        "\n",
        "2. **Loss of Information Due to Discretization:**\n",
        "   - QWK requires discrete predictions, meaning continuous model outputs must be thresholded into distinct categories. This process can obscure valuable information about model performance. For example, QWK does not consider how well items are ranked within each predicted class. Predictions near category thresholds and those far from thresholds are treated equally, potentially masking critical distinctions in model quality.\n",
        "\n",
        "   - A continuous metric might offer a more nuanced and informative assessment of model performance by capturing fine-grained variations within categories. Such an approach could provide deeper insights and lead to better-informed business decisions.\n",
        "\n",
        "These challenges highlight the importance of considering alternative or complementary evaluation metrics to ensure a more comprehensive understanding of model performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9d0c4d1",
      "metadata": {
        "papermill": {
          "duration": 0.020428,
          "end_time": "2024-12-19T13:45:17.137011",
          "exception": false,
          "start_time": "2024-12-19T13:45:17.116583",
          "status": "completed"
        },
        "tags": [],
        "id": "d9d0c4d1"
      },
      "source": [
        "# BASELINE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e84b50",
      "metadata": {
        "papermill": {
          "duration": 0.020601,
          "end_time": "2024-12-19T13:45:17.177783",
          "exception": false,
          "start_time": "2024-12-19T13:45:17.157182",
          "status": "completed"
        },
        "tags": [],
        "id": "40e84b50"
      },
      "source": [
        "---\n",
        "\n",
        "### Issues with the Baseline Approach\n",
        "\n",
        "While the baseline and similar top-performing models on public datasets achieve high scores, they exhibit significant issues that may undermine their reliability:\n",
        "\n",
        "1. **Separate Autoencoder Fitting for Training and Test Sets:**\n",
        "   - The autoencoder is trained independently on the training and test datasets (as seen in the `perform_autoencoder` function). This can lead to inconsistent encodings for the same data, resulting in significant discrepancies between training and test representations.\n",
        "\n",
        "2. **Local Extrema in Threshold Optimization:**\n",
        "   - The threshold optimizer, implemented using `KappaOptimizer = minimize(...)`, often converges to a local extremum rather than the global optimum. This limitation can affect the quality of the thresholding process, reducing the overall effectiveness of the model’s predictions.\n",
        "\n",
        "   - Such a setup raises concerns about the robustness of the baseline, as it may not generalize well to unseen data, particularly in the private test dataset.\n",
        "\n",
        "Interestingly, these questionable techniques have been shown to perform well on public leaderboards. However, their reliability for private datasets remains uncertain. It is hoped that more principled approaches will yield better performance on private datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "158655f0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:45:17.223109Z",
          "iopub.status.busy": "2024-12-19T13:45:17.222241Z",
          "iopub.status.idle": "2024-12-19T13:45:58.816686Z",
          "shell.execute_reply": "2024-12-19T13:45:58.815455Z"
        },
        "papermill": {
          "duration": 41.619656,
          "end_time": "2024-12-19T13:45:58.818935",
          "exception": false,
          "start_time": "2024-12-19T13:45:17.199279",
          "status": "completed"
        },
        "tags": [],
        "id": "158655f0"
      },
      "outputs": [],
      "source": [
        "!pip -q install /kaggle/input/pytorchtabnet/pytorch_tabnet-4.1.0-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235a0111",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:45:58.861979Z",
          "iopub.status.busy": "2024-12-19T13:45:58.861654Z",
          "iopub.status.idle": "2024-12-19T13:45:58.879865Z",
          "shell.execute_reply": "2024-12-19T13:45:58.879090Z"
        },
        "papermill": {
          "duration": 0.041712,
          "end_time": "2024-12-19T13:45:58.881590",
          "exception": false,
          "start_time": "2024-12-19T13:45:58.839878",
          "status": "completed"
        },
        "tags": [],
        "id": "235a0111"
      },
      "outputs": [],
      "source": [
        "from pytorch_tabnet.tab_model import TabNetRegressor\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71e470e7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:45:58.923376Z",
          "iopub.status.busy": "2024-12-19T13:45:58.922865Z",
          "iopub.status.idle": "2024-12-19T13:45:58.929820Z",
          "shell.execute_reply": "2024-12-19T13:45:58.928995Z"
        },
        "papermill": {
          "duration": 0.0297,
          "end_time": "2024-12-19T13:45:58.931410",
          "exception": false,
          "start_time": "2024-12-19T13:45:58.901710",
          "status": "completed"
        },
        "tags": [],
        "id": "71e470e7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.optimize import minimize\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import polars as pl\n",
        "import polars.selectors as cs\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.optimizers import Adam\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from colorama import Fore, Style\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.max_columns = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d32926a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:45:58.972787Z",
          "iopub.status.busy": "2024-12-19T13:45:58.972323Z",
          "iopub.status.idle": "2024-12-19T13:45:58.981621Z",
          "shell.execute_reply": "2024-12-19T13:45:58.980839Z"
        },
        "papermill": {
          "duration": 0.031588,
          "end_time": "2024-12-19T13:45:58.983173",
          "exception": false,
          "start_time": "2024-12-19T13:45:58.951585",
          "status": "completed"
        },
        "tags": [],
        "id": "2d32926a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "seed_everything(2024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "885eb710",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:45:59.025475Z",
          "iopub.status.busy": "2024-12-19T13:45:59.024836Z",
          "iopub.status.idle": "2024-12-19T13:45:59.028311Z",
          "shell.execute_reply": "2024-12-19T13:45:59.027570Z"
        },
        "papermill": {
          "duration": 0.026045,
          "end_time": "2024-12-19T13:45:59.029952",
          "exception": false,
          "start_time": "2024-12-19T13:45:59.003907",
          "status": "completed"
        },
        "tags": [],
        "id": "885eb710"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "n_splits = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78c6e24c",
      "metadata": {
        "papermill": {
          "duration": 0.02071,
          "end_time": "2024-12-19T13:45:59.107069",
          "exception": false,
          "start_time": "2024-12-19T13:45:59.086359",
          "status": "completed"
        },
        "tags": [],
        "id": "78c6e24c"
      },
      "source": [
        "---\n",
        "\n",
        "### Feature Engineering\n",
        "\n",
        "Feature engineering plays a critical role in enhancing model performance by creating and refining variables that capture important aspects of the data. The following techniques were applied:\n",
        "\n",
        "1. **Feature Selection:**\n",
        "   - The dataset includes features related to physical characteristics (e.g., BMI, height, weight), behavioral aspects (e.g., internet usage), and fitness data (e.g., endurance time). Relevant features were selected to maximize predictive power while reducing noise.\n",
        "\n",
        "2. **Categorical Feature Encoding:**\n",
        "   - Categorical variables were converted into numerical values using custom mappings specific to each unique category. This ensured compatibility with machine learning algorithms that require numerical input while preserving the semantic meaning of the categories.\n",
        "\n",
        "3. **Time Series Aggregation:**\n",
        "   - Key statistical measures (e.g., mean, standard deviation) were computed from the actigraphy data and merged into the main dataset. These aggregated features provided additional insights into participants' physical activity patterns, enhancing the dataset’s richness for model training.\n",
        "\n",
        "These strategies ensured that the engineered features captured both individual-level and temporal patterns, contributing to improved model accuracy and interpretability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c16d69f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:45:59.148858Z",
          "iopub.status.busy": "2024-12-19T13:45:59.148260Z",
          "iopub.status.idle": "2024-12-19T13:45:59.162894Z",
          "shell.execute_reply": "2024-12-19T13:45:59.162065Z"
        },
        "papermill": {
          "duration": 0.037291,
          "end_time": "2024-12-19T13:45:59.164577",
          "exception": false,
          "start_time": "2024-12-19T13:45:59.127286",
          "status": "completed"
        },
        "tags": [],
        "id": "5c16d69f"
      },
      "outputs": [],
      "source": [
        "def process_file(filename, dirname):\n",
        "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
        "    df.drop('step', axis=1, inplace=True)\n",
        "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
        "\n",
        "def load_time_series(dirname) -> pd.DataFrame:\n",
        "    ids = os.listdir(dirname)\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
        "\n",
        "    stats, indexes = zip(*results)\n",
        "\n",
        "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
        "    df['id'] = indexes\n",
        "    return df\n",
        "\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, encoding_dim*3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(encoding_dim*2, encoding_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoding_dim, input_dim*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim*2, input_dim*3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(input_dim*3, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "\n",
        "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "    data_tensor = torch.FloatTensor(df_scaled)\n",
        "\n",
        "    input_dim = data_tensor.shape[1]\n",
        "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(autoencoder.parameters())\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, len(data_tensor), batch_size):\n",
        "            batch = data_tensor[i : i + batch_size]\n",
        "            optimizer.zero_grad()\n",
        "            reconstructed = autoencoder(batch)\n",
        "            loss = criterion(reconstructed, batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
        "\n",
        "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
        "\n",
        "    return df_encoded\n",
        "\n",
        "\n",
        "def feature_engineering(df):\n",
        "    season_cols = [col for col in df.columns if 'Season' in col]\n",
        "    df = df.drop(season_cols, axis=1)\n",
        "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
        "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
        "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
        "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
        "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
        "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
        "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
        "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
        "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
        "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
        "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
        "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
        "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
        "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
        "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
        "    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b16e89b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:45:59.207743Z",
          "iopub.status.busy": "2024-12-19T13:45:59.207477Z",
          "iopub.status.idle": "2024-12-19T13:47:29.064912Z",
          "shell.execute_reply": "2024-12-19T13:47:29.063806Z"
        },
        "papermill": {
          "duration": 89.882476,
          "end_time": "2024-12-19T13:47:29.067372",
          "exception": false,
          "start_time": "2024-12-19T13:45:59.184896",
          "status": "completed"
        },
        "tags": [],
        "id": "5b16e89b",
        "outputId": "1a7cc986-1f58-4375-8b08-c65c1c5ab2e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 996/996 [01:11<00:00, 13.99it/s]\n",
            "100%|██████████| 2/2 [00:00<00:00, 13.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/100], Loss: 1.6710]\n",
            "Epoch [20/100], Loss: 1.5469]\n",
            "Epoch [30/100], Loss: 1.5154]\n",
            "Epoch [40/100], Loss: 1.4932]\n",
            "Epoch [50/100], Loss: 1.4964]\n",
            "Epoch [60/100], Loss: 1.4920]\n",
            "Epoch [70/100], Loss: 1.4309]\n",
            "Epoch [80/100], Loss: 1.4185]\n",
            "Epoch [90/100], Loss: 1.3667]\n",
            "Epoch [100/100], Loss: 1.3620]\n",
            "Epoch [10/100], Loss: 1.0070]\n",
            "Epoch [20/100], Loss: 0.5783]\n",
            "Epoch [30/100], Loss: 0.4271]\n",
            "Epoch [40/100], Loss: 0.4271]\n",
            "Epoch [50/100], Loss: 0.4271]\n",
            "Epoch [60/100], Loss: 0.4271]\n",
            "Epoch [70/100], Loss: 0.4271]\n",
            "Epoch [80/100], Loss: 0.4271]\n",
            "Epoch [90/100], Loss: 0.4271]\n",
            "Epoch [100/100], Loss: 0.4271]\n"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
        "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
        "\n",
        "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
        "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
        "\n",
        "df_train = train_ts.drop('id', axis=1)\n",
        "df_test = test_ts.drop('id', axis=1)\n",
        "\n",
        "train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n",
        "test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n",
        "\n",
        "time_series_cols = train_ts_encoded.columns.tolist()\n",
        "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
        "test_ts_encoded['id']=test_ts[\"id\"]\n",
        "\n",
        "train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
        "test = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
        "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
        "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
        "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
        "for col in train.columns:\n",
        "    if col not in numeric_cols:\n",
        "        train_imputed[col] = train[col]\n",
        "\n",
        "train = train_imputed\n",
        "\n",
        "train = feature_engineering(train)\n",
        "train = train.dropna(thresh=10, axis=0)\n",
        "test = feature_engineering(test)\n",
        "\n",
        "train = train.drop('id', axis=1)\n",
        "test  = test .drop('id', axis=1)\n",
        "\n",
        "\n",
        "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
        "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
        "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
        "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
        "                'Fitness_Endurance-Max_Stage',\n",
        "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
        "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
        "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
        "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
        "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
        "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
        "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
        "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
        "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
        "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
        "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
        "                'SDS-SDS_Total_T',\n",
        "                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
        "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
        "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']\n",
        "\n",
        "featuresCols += time_series_cols\n",
        "\n",
        "train = train[featuresCols]\n",
        "train = train.dropna(subset='sii')\n",
        "\n",
        "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
        "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
        "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
        "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
        "                'Fitness_Endurance-Max_Stage',\n",
        "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
        "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
        "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
        "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
        "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
        "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
        "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
        "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
        "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
        "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
        "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
        "                'SDS-SDS_Total_T',\n",
        "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
        "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
        "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW','BMI_PHR']\n",
        "\n",
        "featuresCols += time_series_cols\n",
        "test = test[featuresCols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "395b7ca9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:47:29.145582Z",
          "iopub.status.busy": "2024-12-19T13:47:29.144886Z",
          "iopub.status.idle": "2024-12-19T13:47:29.154415Z",
          "shell.execute_reply": "2024-12-19T13:47:29.153464Z"
        },
        "papermill": {
          "duration": 0.048683,
          "end_time": "2024-12-19T13:47:29.156092",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.107409",
          "status": "completed"
        },
        "tags": [],
        "id": "395b7ca9"
      },
      "outputs": [],
      "source": [
        "if np.any(np.isinf(train)):\n",
        "    train = train.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "def quadratic_weighted_kappa(y_true, y_pred):\n",
        "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
        "\n",
        "def threshold_Rounder(oof_non_rounded, thresholds):\n",
        "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
        "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
        "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
        "\n",
        "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
        "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
        "    return -quadratic_weighted_kappa(y_true, rounded_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "786c249e",
      "metadata": {
        "papermill": {
          "duration": 0.036445,
          "end_time": "2024-12-19T13:47:29.230424",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.193979",
          "status": "completed"
        },
        "tags": [],
        "id": "786c249e"
      },
      "source": [
        "---\n",
        "\n",
        "### Model Training and Evaluation\n",
        "\n",
        "Model training and evaluation were conducted using a variety of algorithms and techniques tailored for the dataset:\n",
        "\n",
        "1. **Model Types:**\n",
        "   - **LightGBM:** A highly efficient gradient-boosting framework optimized for speed and performance on large datasets.\n",
        "   - **XGBoost:** A powerful gradient-boosting model widely used for structured data tasks due to its flexibility and robustness.\n",
        "   - **CatBoost:** Specifically designed for datasets with categorical features, eliminating the need for extensive preprocessing.\n",
        "   - **Voting Regressor:** An ensemble model that combines predictions from LightGBM, XGBoost, and CatBoost to improve accuracy by leveraging their individual strengths.\n",
        "\n",
        "2. **Cross-Validation:**\n",
        "   - Stratified K-Folds cross-validation was used to split the data into training and validation sets. This approach ensures that each fold maintains a balanced class distribution, enabling reliable performance evaluation.\n",
        "\n",
        "3. **Evaluation Metric:**\n",
        "   - The Quadratic Weighted Kappa (QWK) metric was employed to evaluate model performance. QWK is particularly effective for assessing ordinal predictions, accounting for both the magnitude and direction of errors.\n",
        "\n",
        "4. **Threshold Optimization:**\n",
        "   - Continuous model predictions were mapped to discrete ordinal categories (None, Mild, Moderate, Severe) using threshold optimization. The `minimize` function from `scipy.optimize` was utilized to fine-tune these thresholds, aligning predictions with the QWK metric and enhancing classification accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964fda3c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:47:29.304594Z",
          "iopub.status.busy": "2024-12-19T13:47:29.304315Z",
          "iopub.status.idle": "2024-12-19T13:47:29.313348Z",
          "shell.execute_reply": "2024-12-19T13:47:29.312610Z"
        },
        "papermill": {
          "duration": 0.047528,
          "end_time": "2024-12-19T13:47:29.314995",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.267467",
          "status": "completed"
        },
        "tags": [],
        "id": "964fda3c"
      },
      "outputs": [],
      "source": [
        "def TrainML(model_class, test_data):\n",
        "    X = train.drop(['sii'], axis=1)\n",
        "    y = train['sii']\n",
        "\n",
        "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "    train_S = []\n",
        "    test_S = []\n",
        "\n",
        "    oof_non_rounded = np.zeros(len(y), dtype=float)\n",
        "    oof_rounded = np.zeros(len(y), dtype=int)\n",
        "    test_preds = np.zeros((len(test_data), n_splits))\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        model = clone(model_class)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_val_pred = model.predict(X_val)\n",
        "\n",
        "        oof_non_rounded[test_idx] = y_val_pred\n",
        "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
        "        oof_rounded[test_idx] = y_val_pred_rounded\n",
        "\n",
        "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
        "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
        "\n",
        "        train_S.append(train_kappa)\n",
        "        test_S.append(val_kappa)\n",
        "\n",
        "        test_preds[:, fold] = model.predict(test_data)\n",
        "\n",
        "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
        "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
        "\n",
        "    KappaOPtimizer = minimize(evaluate_predictions,\n",
        "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded),\n",
        "                              method='Nelder-Mead')\n",
        "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
        "\n",
        "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
        "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
        "\n",
        "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
        "\n",
        "    tpm = test_preds.mean(axis=1)\n",
        "    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
        "\n",
        "    submission = pd.DataFrame({\n",
        "        'id': sample['id'],\n",
        "        'sii': tpTuned\n",
        "    })\n",
        "\n",
        "    return submission"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "467d2be9",
      "metadata": {
        "papermill": {
          "duration": 0.035664,
          "end_time": "2024-12-19T13:47:29.387503",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.351839",
          "status": "completed"
        },
        "tags": [],
        "id": "467d2be9"
      },
      "source": [
        "---\n",
        "\n",
        "### Hyperparameter Tuning\n",
        "\n",
        "Hyperparameter tuning was performed to optimize the models’ performance by carefully adjusting key parameters:\n",
        "\n",
        "1. **LightGBM Parameters:**\n",
        "   - Key parameters such as `learning_rate`, `max_depth`, `num_leaves`, and `feature_fraction` were tuned to enhance the model's ability to generalize and avoid overfitting. These parameters control the complexity of the model and its learning dynamics.\n",
        "\n",
        "2. **XGBoost and CatBoost Parameters:**\n",
        "   - Similar adjustments were made for XGBoost and CatBoost, focusing on parameters such as `n_estimators`, `max_depth`, `learning_rate`, `subsample`, and regularization terms (`reg_alpha` and `reg_lambda`). These configurations helped balance model complexity and robustness, ensuring consistent performance across diverse data scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db929558",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:47:29.460586Z",
          "iopub.status.busy": "2024-12-19T13:47:29.460275Z",
          "iopub.status.idle": "2024-12-19T13:47:29.465524Z",
          "shell.execute_reply": "2024-12-19T13:47:29.464828Z"
        },
        "papermill": {
          "duration": 0.043797,
          "end_time": "2024-12-19T13:47:29.467084",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.423287",
          "status": "completed"
        },
        "tags": [],
        "id": "db929558"
      },
      "outputs": [],
      "source": [
        "# Model parameters for LightGBM\n",
        "Params = {\n",
        "    'learning_rate': 0.046,\n",
        "    'max_depth': 12,\n",
        "    'num_leaves': 478,\n",
        "    'min_data_in_leaf': 13,\n",
        "    'feature_fraction': 0.893,\n",
        "    'bagging_fraction': 0.784,\n",
        "    'bagging_freq': 4,\n",
        "    'lambda_l1': 10,  # Increased from 6.59\n",
        "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
        "    'device': 'cpu'\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# XGBoost parameters\n",
        "XGB_Params = {\n",
        "    'learning_rate': 0.05,\n",
        "    'max_depth': 6,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'reg_alpha': 1,  # Increased from 0.1\n",
        "    'reg_lambda': 5,  # Increased from 1\n",
        "    'random_state': SEED,\n",
        "    'tree_method': 'gpu_hist',\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "CatBoost_Params = {\n",
        "    'learning_rate': 0.05,\n",
        "    'depth': 6,\n",
        "    'iterations': 200,\n",
        "    'random_seed': SEED,\n",
        "    'verbose': 0,\n",
        "    'l2_leaf_reg': 10,  # Increase this value\n",
        "    'task_type': 'GPU'\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e25fbc1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:47:29.543199Z",
          "iopub.status.busy": "2024-12-19T13:47:29.542966Z",
          "iopub.status.idle": "2024-12-19T13:47:29.557770Z",
          "shell.execute_reply": "2024-12-19T13:47:29.557182Z"
        },
        "papermill": {
          "duration": 0.055069,
          "end_time": "2024-12-19T13:47:29.559389",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.504320",
          "status": "completed"
        },
        "tags": [],
        "id": "5e25fbc1"
      },
      "outputs": [],
      "source": [
        "# New: TabNet\n",
        "\n",
        "from sklearn.base import BaseEstimator, RegressorMixin\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_tabnet.callbacks import Callback\n",
        "import os\n",
        "import torch\n",
        "from pytorch_tabnet.callbacks import Callback\n",
        "\n",
        "\n",
        "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.model = TabNetRegressor(**kwargs)\n",
        "        self.kwargs = kwargs\n",
        "        self.imputer = SimpleImputer(strategy='median')\n",
        "        self.best_model_path = 'best_tabnet_model.pt'\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Handle missing values\n",
        "        X_imputed = self.imputer.fit_transform(X)\n",
        "\n",
        "        if hasattr(y, 'values'):\n",
        "            y = y.values\n",
        "\n",
        "        # Create internal validation set\n",
        "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "            X_imputed,\n",
        "            y,\n",
        "            test_size=0.2,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Train TabNet model\n",
        "        history = self.model.fit(\n",
        "            X_train=X_train,\n",
        "            y_train=y_train.reshape(-1, 1),\n",
        "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
        "            eval_name=['valid'],\n",
        "            eval_metric=['mse'],\n",
        "            max_epochs=200,\n",
        "            patience=20,\n",
        "            batch_size=1024,\n",
        "            virtual_batch_size=128,\n",
        "            num_workers=0,\n",
        "            drop_last=False,\n",
        "            callbacks=[\n",
        "                TabNetPretrainedModelCheckpoint(\n",
        "                    filepath=self.best_model_path,\n",
        "                    monitor='valid_mse',\n",
        "                    mode='min',\n",
        "                    save_best_only=True,\n",
        "                    verbose=True\n",
        "                )\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Load the best model\n",
        "        if os.path.exists(self.best_model_path):\n",
        "            self.model.load_model(self.best_model_path)\n",
        "            os.remove(self.best_model_path)  # Remove temporary file\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        X_imputed = self.imputer.transform(X)\n",
        "        return self.model.predict(X_imputed).flatten()\n",
        "\n",
        "    def __deepcopy__(self, memo):\n",
        "        # Add deepcopy support for scikit-learn\n",
        "        cls = self.__class__\n",
        "        result = cls.__new__(cls)\n",
        "        memo[id(self)] = result\n",
        "        for k, v in self.__dict__.items():\n",
        "            setattr(result, k, deepcopy(v, memo))\n",
        "        return result\n",
        "\n",
        "\n",
        "# TabNet hyperparameters\n",
        "TabNet_Params = {\n",
        "    'n_d': 64,              # Width of the decision prediction layer\n",
        "    'n_a': 64,              # Width of the attention embedding for each step\n",
        "    'n_steps': 5,           # Number of steps in the architecture\n",
        "    'gamma': 1.5,           # Coefficient for feature selection regularization\n",
        "    'n_independent': 2,     # Number of independent GLU layer in each GLU block\n",
        "    'n_shared': 2,          # Number of shared GLU layer in each GLU block\n",
        "    'lambda_sparse': 1e-4,  # Sparsity regularization\n",
        "    'optimizer_fn': torch.optim.Adam,\n",
        "    'optimizer_params': dict(lr=2e-2, weight_decay=1e-5),\n",
        "    'mask_type': 'entmax',\n",
        "    'scheduler_params': dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
        "    'scheduler_fn': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "    'verbose': 1,\n",
        "    'device_name': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "}\n",
        "\n",
        "\n",
        "class TabNetPretrainedModelCheckpoint(Callback):\n",
        "    def __init__(self, filepath, monitor='val_loss', mode='min',\n",
        "                 save_best_only=True, verbose=1):\n",
        "        super().__init__()  # Initialize parent class\n",
        "        self.filepath = filepath\n",
        "        self.monitor = monitor\n",
        "        self.mode = mode\n",
        "        self.save_best_only = save_best_only\n",
        "        self.verbose = verbose\n",
        "        self.best = float('inf') if mode == 'min' else -float('inf')\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.model = self.trainer  # Use trainer itself as model\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        current = logs.get(self.monitor)\n",
        "        if current is None:\n",
        "            return\n",
        "\n",
        "        # Check if current metric is better than best\n",
        "        if (self.mode == 'min' and current < self.best) or \\\n",
        "           (self.mode == 'max' and current > self.best):\n",
        "            if self.verbose:\n",
        "                print(f'\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}')\n",
        "            self.best = current\n",
        "            if self.save_best_only:\n",
        "                self.model.save_model(self.filepath)  # Save the entire model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "572a91d5",
      "metadata": {
        "papermill": {
          "duration": 0.036925,
          "end_time": "2024-12-19T13:47:29.633545",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.596620",
          "status": "completed"
        },
        "tags": [],
        "id": "572a91d5"
      },
      "source": [
        "---\n",
        "\n",
        "### Ensemble Learning and Submission Preparation\n",
        "\n",
        "The final stage of the pipeline involved ensemble learning and the preparation of predictions for submission:\n",
        "\n",
        "1. **Ensemble Learning:**\n",
        "   - A **Voting Regressor** was employed, combining predictions from LightGBM, XGBoost, CatBoost and TabNet. This ensemble approach leverages the strengths of each model, reducing overfitting and improving overall performance.\n",
        "\n",
        "2. **Out-of-Fold (OOF) Predictions:**\n",
        "   - During cross-validation, out-of-fold predictions were generated for the training dataset. These predictions helped evaluate the model’s performance while avoiding data leakage.\n",
        "\n",
        "3. **Kappa Optimizer:**\n",
        "   - The Kappa Optimizer was used to adjust decision thresholds, ensuring that the model’s predictions were as close as possible to the actual target values. This step was crucial for improving classification accuracy in ordinal tasks.\n",
        "\n",
        "4. **Test Set Predictions:**\n",
        "   - Once the ensemble model was trained and the thresholds optimized, predictions were generated for the test dataset. These predictions were converted into class labels using the tuned thresholds.\n",
        "\n",
        "5. **Submission File Creation:**\n",
        "   - The final predictions were formatted into a CSV file suitable for submission. The file included the required columns, such as `id` and `sii` (Severity Impairment Index), ensuring compliance with competition requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56bd7d8e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:47:29.778682Z",
          "iopub.status.busy": "2024-12-19T13:47:29.778406Z",
          "iopub.status.idle": "2024-12-19T13:47:29.788910Z",
          "shell.execute_reply": "2024-12-19T13:47:29.788298Z"
        },
        "papermill": {
          "duration": 0.048701,
          "end_time": "2024-12-19T13:47:29.790472",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.741771",
          "status": "completed"
        },
        "tags": [],
        "id": "56bd7d8e"
      },
      "outputs": [],
      "source": [
        "# Create model instances\n",
        "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
        "XGB_Model = XGBRegressor(**XGB_Params)\n",
        "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
        "TabNet_Model = TabNetWrapper(**TabNet_Params) # New"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d39dbac",
      "metadata": {
        "papermill": {
          "duration": 0.037599,
          "end_time": "2024-12-19T13:47:29.864745",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.827146",
          "status": "completed"
        },
        "tags": [],
        "id": "5d39dbac"
      },
      "source": [
        "---\n",
        "# **》》》Model1. Comprehensive Baseline**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0ec236",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:47:29.939422Z",
          "iopub.status.busy": "2024-12-19T13:47:29.939191Z",
          "iopub.status.idle": "2024-12-19T13:48:45.702493Z",
          "shell.execute_reply": "2024-12-19T13:48:45.701458Z"
        },
        "papermill": {
          "duration": 75.802272,
          "end_time": "2024-12-19T13:48:45.704377",
          "exception": false,
          "start_time": "2024-12-19T13:47:29.902105",
          "status": "completed"
        },
        "tags": [],
        "id": "7b0ec236",
        "outputId": "abf89063-c990-4b97-c6ed-2ea3ca27d356"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Folds: 100%|██████████| 5/5 [01:15<00:00, 15.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Train QWK --> 0.7240\n",
            "Mean Validation QWK ---> 0.4613\n",
            "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.519\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sii</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00008ff9</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000fd460</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00105258</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00115b9f</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0016bb22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>001f3379</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0038ba98</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0068a485</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0069fbed</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0083e397</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0087dd65</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>00abe655</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>00ae59c9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>00af6387</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>00bd4359</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>00c0cd71</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>00d56d4b</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>00d9913d</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>00e6167c</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>00ebc35d</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  sii\n",
              "0   00008ff9    0\n",
              "1   000fd460    0\n",
              "2   00105258    1\n",
              "3   00115b9f    0\n",
              "4   0016bb22    0\n",
              "5   001f3379    1\n",
              "6   0038ba98    1\n",
              "7   0068a485    0\n",
              "8   0069fbed    1\n",
              "9   0083e397    0\n",
              "10  0087dd65    0\n",
              "11  00abe655    0\n",
              "12  00ae59c9    1\n",
              "13  00af6387    1\n",
              "14  00bd4359    1\n",
              "15  00c0cd71    1\n",
              "16  00d56d4b    0\n",
              "17  00d9913d    0\n",
              "18  00e6167c    0\n",
              "19  00ebc35d    1"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "voting_model = VotingRegressor(estimators=[\n",
        "    ('lightgbm', Light),\n",
        "    ('xgboost', XGB_Model),\n",
        "    ('catboost', CatBoost_Model),\n",
        "    ('tabnet', TabNet_Model)\n",
        "],weights=[4.0,4.0,5.0,4.0])\n",
        "\n",
        "Submission1 = TrainML(voting_model, test)\n",
        "\n",
        "Submission1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a9345e",
      "metadata": {
        "papermill": {
          "duration": 0.038416,
          "end_time": "2024-12-19T13:48:45.788593",
          "exception": false,
          "start_time": "2024-12-19T13:48:45.750177",
          "status": "completed"
        },
        "tags": [],
        "id": "c1a9345e"
      },
      "source": [
        "```\n",
        "],weights=[5.0,4.0,4.0,4.0])\n",
        "Mean Train QWK --> 0.7424\n",
        "Mean Validation QWK ---> 0.4735\n",
        "----> || Optimized QWK SCORE ::  0.533\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afb47a8d",
      "metadata": {
        "papermill": {
          "duration": 0.037287,
          "end_time": "2024-12-19T13:50:49.253404",
          "exception": false,
          "start_time": "2024-12-19T13:50:49.216117",
          "status": "completed"
        },
        "tags": [],
        "id": "afb47a8d"
      },
      "source": [
        "---\n",
        "# **》》》Model3. Backup Baseline**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee5156c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:50:49.332051Z",
          "iopub.status.busy": "2024-12-19T13:50:49.331757Z",
          "iopub.status.idle": "2024-12-19T13:54:02.257223Z",
          "shell.execute_reply": "2024-12-19T13:54:02.256401Z"
        },
        "papermill": {
          "duration": 192.966546,
          "end_time": "2024-12-19T13:54:02.259015",
          "exception": false,
          "start_time": "2024-12-19T13:50:49.292469",
          "status": "completed"
        },
        "tags": [],
        "id": "2ee5156c",
        "outputId": "6f9e2021-3202-4344-9cd3-a3c7eda5151f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Folds: 100%|██████████| 5/5 [02:01<00:00, 24.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Train QWK --> 0.9175\n",
            "Mean Validation QWK ---> 0.3803\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----> || Optimized QWK SCORE :: \u001b[36m\u001b[1m 0.450\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sii</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00008ff9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000fd460</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00105258</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00115b9f</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0016bb22</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>001f3379</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0038ba98</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0068a485</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0069fbed</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0083e397</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0087dd65</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>00abe655</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>00ae59c9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>00af6387</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>00bd4359</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>00c0cd71</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>00d56d4b</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>00d9913d</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>00e6167c</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>00ebc35d</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  sii\n",
              "0   00008ff9    2\n",
              "1   000fd460    0\n",
              "2   00105258    0\n",
              "3   00115b9f    0\n",
              "4   0016bb22    1\n",
              "5   001f3379    1\n",
              "6   0038ba98    0\n",
              "7   0068a485    0\n",
              "8   0069fbed    2\n",
              "9   0083e397    0\n",
              "10  0087dd65    1\n",
              "11  00abe655    0\n",
              "12  00ae59c9    2\n",
              "13  00af6387    1\n",
              "14  00bd4359    2\n",
              "15  00c0cd71    2\n",
              "16  00d56d4b    0\n",
              "17  00d9913d    0\n",
              "18  00e6167c    0\n",
              "19  00ebc35d    1"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
        "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
        "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
        "\n",
        "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
        "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
        "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
        "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
        "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
        "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
        "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
        "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
        "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
        "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
        "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
        "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
        "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
        "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
        "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
        "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
        "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
        "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
        "\n",
        "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season',\n",
        "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season',\n",
        "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
        "\n",
        "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
        "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
        "\n",
        "time_series_cols = train_ts.columns.tolist()\n",
        "time_series_cols.remove(\"id\")\n",
        "\n",
        "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
        "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
        "\n",
        "train = train.drop('id', axis=1)\n",
        "test = test.drop('id', axis=1)\n",
        "\n",
        "featuresCols += time_series_cols\n",
        "\n",
        "train = train[featuresCols]\n",
        "train = train.dropna(subset='sii')\n",
        "\n",
        "def update(df):\n",
        "    global cat_c\n",
        "    for c in cat_c:\n",
        "        df[c] = df[c].fillna('Missing')\n",
        "        df[c] = df[c].astype('category')\n",
        "    return df\n",
        "\n",
        "train = update(train)\n",
        "test = update(test)\n",
        "\n",
        "def create_mapping(column, dataset):\n",
        "    unique_values = dataset[column].unique()\n",
        "    return {value: idx for idx, value in enumerate(unique_values)}\n",
        "\n",
        "for col in cat_c:\n",
        "    mapping = create_mapping(col, train)\n",
        "    mappingTe = create_mapping(col, test)\n",
        "\n",
        "    train[col] = train[col].replace(mapping).astype(int)\n",
        "    test[col] = test[col].replace(mappingTe).astype(int)\n",
        "\n",
        "def quadratic_weighted_kappa(y_true, y_pred):\n",
        "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
        "\n",
        "def threshold_Rounder(oof_non_rounded, thresholds):\n",
        "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
        "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
        "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
        "\n",
        "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
        "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
        "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
        "\n",
        "def TrainML(model_class, test_data):\n",
        "    X = train.drop(['sii'], axis=1)\n",
        "    y = train['sii']\n",
        "\n",
        "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "    train_S = []\n",
        "    test_S = []\n",
        "\n",
        "    oof_non_rounded = np.zeros(len(y), dtype=float)\n",
        "    oof_rounded = np.zeros(len(y), dtype=int)\n",
        "    test_preds = np.zeros((len(test_data), n_splits))\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
        "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        model = clone(model_class)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_val_pred = model.predict(X_val)\n",
        "\n",
        "        oof_non_rounded[test_idx] = y_val_pred\n",
        "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
        "        oof_rounded[test_idx] = y_val_pred_rounded\n",
        "\n",
        "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
        "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
        "\n",
        "        train_S.append(train_kappa)\n",
        "        test_S.append(val_kappa)\n",
        "\n",
        "        test_preds[:, fold] = model.predict(test_data)\n",
        "\n",
        "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
        "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
        "\n",
        "    KappaOPtimizer = minimize(evaluate_predictions,\n",
        "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded),\n",
        "                              method='Nelder-Mead')\n",
        "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
        "    thresholds = KappaOPtimizer.x\n",
        "\n",
        "    oof_tuned = threshold_Rounder(oof_non_rounded, thresholds)\n",
        "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
        "\n",
        "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
        "\n",
        "    tpm = test_preds.mean(axis=1)\n",
        "    tp_rounded = threshold_Rounder(tpm, thresholds)\n",
        "\n",
        "    return tp_rounded\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "\n",
        "ensemble = VotingRegressor(estimators=[\n",
        "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
        "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
        "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
        "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
        "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n",
        "])\n",
        "\n",
        "Submission3 = TrainML(ensemble, test)\n",
        "Submission3 = pd.DataFrame({\n",
        "    'id': sample['id'],\n",
        "    'sii': Submission3\n",
        "})\n",
        "\n",
        "Submission3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "895afbb6",
      "metadata": {
        "papermill": {
          "duration": 0.037625,
          "end_time": "2024-12-19T13:54:02.334897",
          "exception": false,
          "start_time": "2024-12-19T13:54:02.297272",
          "status": "completed"
        },
        "tags": [],
        "id": "895afbb6"
      },
      "source": [
        "# Blending Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fefa879",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:54:02.412580Z",
          "iopub.status.busy": "2024-12-19T13:54:02.411902Z",
          "iopub.status.idle": "2024-12-19T13:54:02.439529Z",
          "shell.execute_reply": "2024-12-19T13:54:02.438751Z"
        },
        "papermill": {
          "duration": 0.068994,
          "end_time": "2024-12-19T13:54:02.441147",
          "exception": false,
          "start_time": "2024-12-19T13:54:02.372153",
          "status": "completed"
        },
        "tags": [],
        "id": "9fefa879",
        "outputId": "f303e704-4108-4ae2-efaa-f8913cfd8e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Majority voting completed and saved to 'submission.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "sub1 = Submission1\n",
        "#sub2 = Submission2\n",
        "sub3 = Submission3\n",
        "\n",
        "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
        "#sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
        "sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n",
        "\n",
        "combined = pd.DataFrame({\n",
        "    'id': sub1['id'],\n",
        "    'sii_1': sub1['sii'],\n",
        "    #'sii_2': sub2['sii'],\n",
        "    'sii_3': sub3['sii']\n",
        "})\n",
        "\n",
        "def majority_vote(row):\n",
        "    return row.mode()[0]\n",
        "\n",
        "def majority_vote_with_fallback(row):\n",
        "    # Lấy các dự đoán từ 3 mô hình\n",
        "    votes = row[['sii_1', 'sii_3']]\n",
        "    #votes = row[['sii_1', 'sii_2', 'sii_3']]\n",
        "\n",
        "    # Tìm các giá trị xuất hiện nhiều nhất\n",
        "    mode = votes.mode()\n",
        "\n",
        "    # Nếu chỉ có 1 giá trị xuất hiện nhiều nhất, chọn nó\n",
        "    if len(mode) == 1:\n",
        "        return mode[0]\n",
        "    else:\n",
        "        return round(row[['sii_1', 'sii_3']].mean()) #[$$$] Mean\n",
        "        #return round(row[['sii_1', 'sii_2', 'sii_3']].mean())\n",
        "\n",
        "\n",
        "# Majority voting\n",
        "combined['final_sii'] = combined[['sii_1', 'sii_3']].apply(majority_vote_with_fallback, axis=1)\n",
        "#combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote_with_fallback, axis=1)\n",
        "\n",
        "# Tạo final_submission và lưu vào file CSV\n",
        "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
        "final_submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Majority voting completed and saved to 'submission.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "465e088c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-19T13:54:02.520970Z",
          "iopub.status.busy": "2024-12-19T13:54:02.520715Z",
          "iopub.status.idle": "2024-12-19T13:54:02.528346Z",
          "shell.execute_reply": "2024-12-19T13:54:02.527627Z"
        },
        "papermill": {
          "duration": 0.048501,
          "end_time": "2024-12-19T13:54:02.530574",
          "exception": false,
          "start_time": "2024-12-19T13:54:02.482073",
          "status": "completed"
        },
        "tags": [],
        "id": "465e088c",
        "outputId": "444f41cf-d916-430d-9d2d-0a5a4444c6f1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sii</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00008ff9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000fd460</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00105258</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00115b9f</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0016bb22</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>001f3379</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0038ba98</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0068a485</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0069fbed</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0083e397</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0087dd65</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>00abe655</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>00ae59c9</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>00af6387</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>00bd4359</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>00c0cd71</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>00d56d4b</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>00d9913d</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>00e6167c</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>00ebc35d</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  sii\n",
              "0   00008ff9    1\n",
              "1   000fd460    0\n",
              "2   00105258    0\n",
              "3   00115b9f    0\n",
              "4   0016bb22    0\n",
              "5   001f3379    1\n",
              "6   0038ba98    0\n",
              "7   0068a485    0\n",
              "8   0069fbed    2\n",
              "9   0083e397    0\n",
              "10  0087dd65    0\n",
              "11  00abe655    0\n",
              "12  00ae59c9    2\n",
              "13  00af6387    1\n",
              "14  00bd4359    2\n",
              "15  00c0cd71    2\n",
              "16  00d56d4b    0\n",
              "17  00d9913d    0\n",
              "18  00e6167c    0\n",
              "19  00ebc35d    1"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_submission"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 9643020,
          "sourceId": 81933,
          "sourceType": "competition"
        },
        {
          "datasetId": 921302,
          "sourceId": 7453542,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30762,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 671.247011,
      "end_time": "2024-12-19T13:54:05.532468",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-12-19T13:42:54.285457",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}